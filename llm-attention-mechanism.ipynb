{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16189fd2",
   "metadata": {},
   "source": [
    "## LLM: Attention Mechanism\n",
    "\n",
    "4 types of attention mechanism\n",
    "1. Simplified attention\n",
    "    - simplified self attention technique\n",
    "2. Self attention\n",
    "    - self attention with trainable weights\n",
    "3. Casual attention\n",
    "    - self attention that allows model to only consider previous and current inputs in a sequence\n",
    "4. Multihead attention\n",
    "    - an extension of self attention and causal attention that enables the model to simultaneously attend the information from different representative subspace\n",
    "    - LLM attends to various input data in parallel\n",
    "\n",
    "\n",
    "The problem with modeling long sequence:\n",
    "- in case of language translation, translation process require contextual understanding and grammar alignment. \n",
    "- this is not possible when we try to perform word to word translation.\n",
    "- One solution was to utilise a neural network with two sub modules, an encoder and a decoder\n",
    "    - encoder reads and processes the text into a context vector\n",
    "    - decoder translate the text using the context vector \n",
    "- before transformers, RNN were used the most popular encoder and decoder architecture\n",
    "    - RNN or recurrent neural network: output from previous step is fed as input to the current model\n",
    "\n",
    "steps of RNN encoder decoder architecture \n",
    "1. Input text\n",
    "2. Enoder (processes input text sequentially)\n",
    "3. Updates hidden state at each step ( internal values at hidden layers )\n",
    "4. final hidden state ( encoder tries to capture sentence meaning )\n",
    "5. Decoder uses this final hidden state to generate translated sentence ( decoder also updates its hidden state at each time )\n",
    "\n",
    "- the encoder processes the entire tezt into a hidden state ( memory cell ). Decoder takes this hidden state to produce an output\n",
    "- Big Issue: \n",
    "    - RNN cannot directly access earlier hidden states from the encoder during the encoding phase\n",
    "    - it relies solely on the current hidden state\n",
    "    - this lead to a loss of context, especially in complex sentences where dependencies might span long distance\n",
    "    - Difficult for RNN to capture all information in a single vector\n",
    "- Capturing data dependencies with attention mechanism \n",
    "- in RNN, the decoder does not have previous words as input along with the context vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857c6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
